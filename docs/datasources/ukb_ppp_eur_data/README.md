# UK Biobank Pharma Proteomics Project (UKB-PPP)

This document was updated on 2024-10-01

Data source comes from the `https://registry.opendata.aws/ukbppp/`

Data stored under `gs://ukb_ppp_eur_data` bucket comes with following structure

```
gs://ukb_ppp_eur_data/clumped/ <- legacy
gs://ukb_ppp_eur_data/collected/ <- legacy
gs://ukb_ppp_eur_data/credible_set_datasets/
gs://ukb_ppp_eur_data/finemapped/ <- legacy
gs://ukb_ppp_eur_data/finemapped_20240719/ <- legacy
gs://ukb_ppp_eur_data/finemapped_20240919/ <- legacy
gs://ukb_ppp_eur_data/finemapping_logs/
gs://ukb_ppp_eur_data/study_index/
gs://ukb_ppp_eur_data/summary_stats/
```

## summary_stats

This directory contains harmonised `summary_statistics` from the data that

The data for the staging bucket is generated by two dags:

## Note about the pre-steps

### 1. Mirror

- **Input.** Original data is hosted on Synapse.
- **Transformation.**
  - As we decided in the past, we want to keep the copy of the original data in the Vault.
  - Protocol is available here: https://github.com/opentargets/gentropy-vault/blob/main/datasets/ukb-ppp.md.
  - The protocol must be run manually.
- **Output.** The output of this step is kept forever in the Vault.

### 2. Preprocess

- **Input.** The mirrored data from the previous step.
- **Transformation.**
  - The data which we mirrored during the previous steps has to undergo several specific transformations which aren't achievable in Spark (especially the first one):
    - Extract gzipped per-chromosome files from inside the individual TAR files, uncompress, partition by chromosome
    - Recreate the study ID. This is required because multiple rows in the study index can reuse the same summary stats file
    - Drop certain rows which don't have a corresponding summary stats file
  - This transformation is done using Google Batch. The code can be found in this new repository: https://github.com/opentargets/gentropy-input-support. The UKB PPP-specific part is this one: https://github.com/opentargets/gentropy-input-support/blob/dc5f8f7aee13a066933f3fd5b18a9b3a5ca71069/data_sources.py#L43-L103.
  - The command to run is `./submit.py ukb_ppp_eur` inside the `gentropy-input-support`
  - This step must also be triggered manually, how to do this is described in the repository.
- **Output.** Precursors of study index and summary stats datasets are output. Because we decided that we don't want to keep the data twice, the output of this step is only kept temporarily and is deleted after 60 days according to the _gs://gentropy-tmp_ bucket lifecycle rules.

### 3. Ingest, harmonise & finemap

All steps run by two dags:

1. `ukb_ppp_eur_ingestion`
2. `ukb_ppp_eur_finemapping`

#### Ingestion step

The ingestion step does not start from the raw data, instead it starts from the pre-initialized data. The pre-initialized data should be created as described in the step 2.
These files consists of:

- gs://gentropy-tmp/batch/output/ukb_ppp_eur/study_index.tsv
- gs://gentropy-tmp/batch/output/ukb_ppp_eur/summary_stats.parquet

The ingestion step then creates 3 datasets:

1. `study_index`,
2. `summary_statistics`
3. `study_locus`

and performs QC over the obtained `summary_statistics`.

#### Finemapping step

The finemapping step should be run after the harmonisation step.
This step will result in performing susie-inf finemapping on the `study_locus` dataset to output `credible_sets`.

The format of input and output is `study_locus` object.
